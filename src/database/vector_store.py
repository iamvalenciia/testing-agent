"""
╔══════════════════════════════════════════════════════════════════════════════╗
║                         PINECONE VECTOR STORE SERVICE                        ║
║                                                                              ║
║  This module implements a vector database interface using Pinecone for       ║
║  storing and retrieving document embeddings generated by Gemini's            ║
║  gemini-embedding-001 model.                                                 ║
╚══════════════════════════════════════════════════════════════════════════════╝

ARCHITECTURE OVERVIEW
=====================

┌─────────────────────────────────────────────────────────────────────────────┐
│                           PINECONE DATA MODEL                               │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Pinecone is a VECTOR DATABASE, not a relational database.                  │
│  It stores data as VECTORS with associated METADATA.                        │
│                                                                             │
│  Each record in Pinecone has 3 components:                                  │
│                                                                             │
│  ┌──────────────┬───────────────────────────┬──────────────────────────┐   │
│  │     ID       │        VECTOR             │       METADATA           │   │
│  │  (string)    │   (float array)           │   (JSON object)          │   │
│  ├──────────────┼───────────────────────────┼──────────────────────────┤   │
│  │ "overrides_  │ [0.023, -0.045, 0.012,    │ {                        │   │
│  │  row_15"     │  ..., 0.089]              │   "sheet_name": "Over..",│   │
│  │              │  (1536 dimensions)        │   "row_index": 15,       │   │
│  │              │                           │   "source": "hammer_xl", │   │
│  │              │                           │   "Question": "What...", │   │
│  │              │                           │   "Answer": "The ans..." │   │
│  │              │                           │ }                        │   │
│  └──────────────┴───────────────────────────┴──────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

HOW SHEET/ROW RELATIONSHIPS ARE MAINTAINED
===========================================

Since Pinecone is NOT relational, we maintain relationships through:

1. VECTOR ID NAMING CONVENTION:
   ┌────────────────────────────────────────────────────────────────────┐
   │  ID = "{sheet_name_normalized}_row_{row_index}"                    │
   │                                                                    │
   │  Examples:                                                         │
   │    • "group_definitions_row_0"                                     │
   │    • "master_question_list_row_42"                                 │
   │    • "overrides_row_156"                                           │
   │    • "triggers_row_3"                                              │
   │                                                                    │
   │  This allows us to:                                                │
   │    ✓ Identify which sheet a vector came from                       │
   │    ✓ Know the exact row in the original Excel                      │
   │    ✓ Guarantee uniqueness across all sheets                        │
   └────────────────────────────────────────────────────────────────────┘

2. METADATA STORAGE:
   ┌────────────────────────────────────────────────────────────────────┐
   │  Each vector stores rich metadata including:                       │
   │                                                                    │
   │  {                                                                 │
   │    "source": "hammer_excel",      // Data source identifier        │
   │    "sheet_name": "Overrides",     // Original Excel sheet name     │
   │    "row_index": 156,              // Row number in the sheet       │
   │    "Column1": "value1",           // All non-empty column values   │
   │    "Column2": "value2",           // from the Excel row            │
   │    ...                                                             │
   │  }                                                                 │
   │                                                                    │
   │  Benefits:                                                         │
   │    ✓ Full traceability back to source                              │
   │    ✓ Can filter queries by sheet_name                              │
   │    ✓ Original data preserved for display                           │
   └────────────────────────────────────────────────────────────────────┘

3. SEMANTIC TEXT EMBEDDING:
   ┌────────────────────────────────────────────────────────────────────┐
   │  The VECTOR itself encodes the semantic meaning of the row:        │
   │                                                                    │   
   │  Text: "Sheet: Overrides. Question: How to configure X?.           │
   │         Answer: You need to set Y to Z. Category: Settings"        │
   │                     │                                              │
   │                     ▼                                              │
   │         ┌─────────────────────┐                                    │
   │         │   Gemini Embedding  │                                    │
   │         │   gemini-embedding-001 │                                 │
   │         └─────────────────────┘                                    │
   │                     │                                              │
   │                     ▼                                              │
   │         [0.023, -0.045, ..., 0.089]  (1536 floats)                 │
   │                                                                    │
   │  This vector captures the MEANING, allowing semantic search.       │
   └────────────────────────────────────────────────────────────────────┘

QUERY FLOW
==========

┌─────────────┐     ┌──────────────┐     ┌─────────────┐     ┌──────────────┐
│ User Query  │────▶│ Gemini API   │────▶│  Pinecone   │────▶│   Results    │
│ "override   │     │ (RETRIEVAL_  │     │  Cosine     │     │ with scores  │
│  settings"  │     │  QUERY)      │     │  Similarity │     │ & metadata   │
└─────────────┘     └──────────────┘     └─────────────┘     └──────────────┘
                           │                    │
                           ▼                    ▼
                    Query embedding      Compare against all
                    [0.01, -0.03, ...]   stored vectors
                    (1536 dims)          Return top K matches

"""
import time
from google import genai
from google.genai import types
import numpy as np
from pinecone import Pinecone, ServerlessSpec
from src.config import Config


class PineconeService:
    """
    Vector Store Service for Pinecone with Gemini Embeddings.
    
    This service provides a complete interface for:
    - Creating and managing Pinecone indexes
    - Generating embeddings using Gemini's gemini-embedding-001 model
    - Upserting, querying, and deleting vectors
    
    The service uses task-type optimization:
    - RETRIEVAL_DOCUMENT: For indexing documents (asymmetric retrieval)
    - RETRIEVAL_QUERY: For search queries (paired with RETRIEVAL_DOCUMENT)
    
    Attributes:
        EMBEDDING_MODEL (str): Gemini model for embeddings
        EMBEDDING_DIMENSION (int): Vector dimension (768, 1536, or 3072)
    """
    
    # ═══════════════════════════════════════════════════════════════════════
    # CONFIGURATION CONSTANTS
    # ═══════════════════════════════════════════════════════════════════════
    
    EMBEDDING_MODEL = "gemini-embedding-001"
    EMBEDDING_DIMENSION = 1536  # Options: 768 (fast), 1536 (balanced), 3072 (best)
    
    # Batch sizes for API limits
    EMBEDDING_BATCH_SIZE = 50   # Gemini API limit per request
    UPSERT_BATCH_SIZE = 100     # Pinecone recommended batch size
    DELETE_BATCH_SIZE = 1000    # Pinecone delete batch size
    
    def __init__(self):
        """
        Initialize the Pinecone service.
        
        This constructor:
        1. Validates required environment variables
        2. Connects to Pinecone using API key
        3. Initializes Gemini client for embeddings
        4. Ensures the index exists (creates if needed)
        """
        Config.validate()
        self.pc = Pinecone(api_key=Config.PINECONE_API_KEY)
        self.index_name = Config.PINECONE_INDEX_NAME
        self.client = genai.Client(api_key=Config.GOOGLE_API_KEY)
        self._ensure_index_exists()
        self.index = self.pc.Index(self.index_name)

    # ═══════════════════════════════════════════════════════════════════════
    # INDEX MANAGEMENT
    # ═══════════════════════════════════════════════════════════════════════

    def _ensure_index_exists(self):
        """
        Ensure the Pinecone index exists, creating it if necessary.
        
        Index Configuration:
        - Dimension: 1536 (matches Gemini embedding output)
        - Metric: Cosine similarity (best for semantic search)
        - Spec: Serverless on AWS us-east-1
        """
        existing_indexes = [index.name for index in self.pc.list_indexes()]
        
        if self.index_name not in existing_indexes:
            print(f"Creating Pinecone index: {self.index_name}...")
            self.pc.create_index(
                name=self.index_name,
                dimension=self.EMBEDDING_DIMENSION,
                metric="cosine",
                spec=ServerlessSpec(cloud="aws", region="us-east-1")
            )
            # Wait for index to be ready
            while not self.pc.describe_index(self.index_name).status['ready']:
                time.sleep(1)
            print(f"Index {self.index_name} is ready.")

    def get_index_stats(self) -> dict:
        """
        Get statistics about the current index.
        
        Returns:
            dict: Index statistics including:
                - total_vector_count: Total vectors stored
                - dimension: Vector dimension
                - namespaces: Namespace breakdown (if used)
        """
        stats = self.index.describe_index_stats()
        return {
            "total_vector_count": stats.total_vector_count,
            "dimension": stats.dimension,
            "index_fullness": stats.index_fullness,
            "namespaces": stats.namespaces
        }

    def delete_all_vectors(self) -> int:
        """
        Delete ALL vectors from the index.
        
        ┌─────────────────────────────────────────────────────────────────┐
        │  ⚠️  WARNING: This is a DESTRUCTIVE operation!                  │
        │                                                                 │
        │  This method removes ALL vectors from the index, effectively    │
        │  clearing the entire database. Use before re-ingestion to       │
        │  ensure a clean state.                                          │
        │                                                                 │
        │  Process:                                                       │
        │  1. Get current vector count                                    │
        │  2. Delete all vectors using delete(delete_all=True)            │
        │  3. Wait for deletion to propagate                              │
        │  4. Verify deletion                                             │
        └─────────────────────────────────────────────────────────────────┘
        
        Returns:
            int: Number of vectors that were deleted
            
        Example:
            >>> service = PineconeService()
            >>> deleted = service.delete_all_vectors()
            >>> print(f"Deleted {deleted} vectors")
        """
        # Get current count before deletion
        stats_before = self.index.describe_index_stats()
        count_before = stats_before.total_vector_count
        
        if count_before == 0:
            print("Index is already empty. No vectors to delete.")
            return 0
        
        print(f"Deleting {count_before} vectors from index '{self.index_name}'...")
        
        # Delete all vectors - Pinecone's delete_all is the most efficient method
        self.index.delete(delete_all=True)
        
        # Wait for deletion to propagate (eventual consistency)
        print("Waiting for deletion to propagate...")
        time.sleep(2)
        
        # Verify deletion
        stats_after = self.index.describe_index_stats()
        count_after = stats_after.total_vector_count
        
        deleted_count = count_before - count_after
        print(f"✓ Deleted {deleted_count} vectors. Current count: {count_after}")
        
        return deleted_count

    def delete_vectors_by_prefix(self, prefix: str) -> int:
        """
        Delete vectors whose IDs start with a specific prefix.
        
        Useful for deleting all vectors from a specific sheet:
            delete_vectors_by_prefix("overrides_")  # Deletes all Override rows
            delete_vectors_by_prefix("triggers_")   # Deletes all Trigger rows
        
        Args:
            prefix: ID prefix to match (e.g., "overrides_", "triggers_")
            
        Returns:
            int: Approximate number of vectors deleted
            
        Note:
            Pinecone Serverless doesn't support prefix-based deletion directly.
            We must fetch IDs first, then delete in batches.
        """
        print(f"Fetching vectors with prefix '{prefix}'...")
        
        # For serverless, we need to query for IDs first
        # This is a workaround since delete(filter=) has limitations
        deleted_count = 0
        
        # Use list() to get all vectors with the prefix
        # Note: This requires iterating through the index
        try:
            # Get vector IDs by querying with a dummy vector and high top_k
            # This is a workaround for listing vectors
            stats = self.index.describe_index_stats()
            total = stats.total_vector_count
            
            if total == 0:
                print("Index is empty.")
                return 0
            
            # Query to find matching IDs (limited approach)
            # For production, consider using namespaces instead
            print(f"Note: Prefix deletion on serverless requires alternative approaches.")
            print(f"Consider using delete_all_vectors() and re-ingesting specific sheets.")
            
        except Exception as e:
            print(f"Error during prefix deletion: {e}")
        
        return deleted_count

    # ═══════════════════════════════════════════════════════════════════════
    # EMBEDDING GENERATION
    # ═══════════════════════════════════════════════════════════════════════

    def _normalize_embedding(self, embedding_values: np.ndarray) -> list[float]:
        """
        Normalize embedding vector to unit length.
        
        Normalization is required for non-3072 dimension embeddings to ensure
        accurate cosine similarity calculations. A normalized vector has 
        magnitude (L2 norm) of 1.0.
        
        Mathematical formula:
            v_normalized = v / ||v||
            where ||v|| = sqrt(sum(v_i^2))
        
        Args:
            embedding_values: Raw embedding from Gemini API
            
        Returns:
            list[float]: Normalized embedding with ||v|| ≈ 1.0
        """
        norm = np.linalg.norm(embedding_values)
        if norm == 0:
            return embedding_values.tolist()
        normed = embedding_values / norm
        return normed.tolist()

    def embed_text(self, text: str, task_type: str = "RETRIEVAL_DOCUMENT") -> list[float]:
        """
        Generate embedding for a single text string.
        
        Task Type Selection Guide:
        ┌─────────────────────┬──────────────────────────────────────────┐
        │ Task Type           │ Use Case                                 │
        ├─────────────────────┼──────────────────────────────────────────┤
        │ RETRIEVAL_DOCUMENT  │ Documents to be stored and retrieved     │
        │ RETRIEVAL_QUERY     │ Search queries to find documents         │
        │ SEMANTIC_SIMILARITY │ Comparing similarity between texts       │
        │ CLASSIFICATION      │ Categorizing text into classes           │
        │ CLUSTERING          │ Grouping similar texts together          │
        └─────────────────────┴──────────────────────────────────────────┘
        
        Args:
            text: Text to embed
            task_type: Embedding optimization type
            
        Returns:
            list[float]: Normalized 1536-dimension embedding vector
        """
        result = self.client.models.embed_content(
            model=self.EMBEDDING_MODEL,
            contents=text,
            config=types.EmbedContentConfig(
                task_type=task_type,
                output_dimensionality=self.EMBEDDING_DIMENSION
            )
        )
        if not result.embeddings:
            return []
        
        embedding_values = np.array(result.embeddings[0].values)
        return self._normalize_embedding(embedding_values)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """
        Generate embeddings for multiple documents with batching.
        
        This method handles large document sets by:
        1. Splitting into batches of 50 (Gemini API limit)
        2. Processing each batch sequentially
        3. Aggregating all embeddings
        
        Uses RETRIEVAL_DOCUMENT task type optimized for asymmetric retrieval
        (documents are indexed, queries search against them).
        
        Args:
            texts: List of document texts to embed
            
        Returns:
            list[list[float]]: List of normalized embedding vectors
        """
        if not texts:
            return []
        
        all_embeddings = []
        total_batches = (len(texts) - 1) // self.EMBEDDING_BATCH_SIZE + 1
        
        for i in range(0, len(texts), self.EMBEDDING_BATCH_SIZE):
            batch = texts[i:i + self.EMBEDDING_BATCH_SIZE]
            batch_num = i // self.EMBEDDING_BATCH_SIZE + 1
            print(f"Embedding batch {batch_num}/{total_batches}...")
            
            result = self.client.models.embed_content(
                model=self.EMBEDDING_MODEL,
                contents=batch,
                config=types.EmbedContentConfig(
                    task_type="RETRIEVAL_DOCUMENT",
                    output_dimensionality=self.EMBEDDING_DIMENSION
                )
            )
            
            if result.embeddings:
                for embedding_obj in result.embeddings:
                    embedding_values = np.array(embedding_obj.values)
                    all_embeddings.append(self._normalize_embedding(embedding_values))
        
        return all_embeddings

    def embed_query(self, query: str) -> list[float]:
        """
        Generate embedding for a search query.
        
        Uses RETRIEVAL_QUERY task type which is specifically optimized for
        finding relevant documents. This is the counterpart to RETRIEVAL_DOCUMENT
        used during indexing (asymmetric retrieval pattern).
        
        Args:
            query: Search query text
            
        Returns:
            list[float]: Normalized query embedding
        """
        return self.embed_text(query, task_type="RETRIEVAL_QUERY")

    # ═══════════════════════════════════════════════════════════════════════
    # VECTOR OPERATIONS
    # ═══════════════════════════════════════════════════════════════════════

    def upsert_vectors(self, vectors: list[tuple]):
        """
        Upsert (insert or update) vectors into Pinecone.
        
        Vector Format:
        ┌────────────────────────────────────────────────────────────────┐
        │  Each vector is a tuple of 3 elements:                         │
        │                                                                │
        │  (id, embedding, metadata)                                     │
        │   │      │           │                                         │
        │   │      │           └─► dict: {"sheet_name": "X", ...}        │
        │   │      └─────────────► list[float]: [0.01, -0.02, ...]       │
        │   └────────────────────► str: "sheet_name_row_index"           │
        └────────────────────────────────────────────────────────────────┘
        
        The 'upsert' operation:
        - INSERT if ID doesn't exist
        - UPDATE (replace) if ID already exists
        
        This makes re-ingestion idempotent - running twice produces the
        same result as running once.
        
        Args:
            vectors: List of (id, embedding, metadata) tuples
        """
        total_batches = (len(vectors) - 1) // self.UPSERT_BATCH_SIZE + 1
        
        for i in range(0, len(vectors), self.UPSERT_BATCH_SIZE):
            batch = vectors[i:i + self.UPSERT_BATCH_SIZE]
            self.index.upsert(vectors=batch)
            batch_num = i // self.UPSERT_BATCH_SIZE + 1
            print(f"Upserted batch {batch_num}/{total_batches} ({len(batch)} vectors)")

    def query(self, query_text: str, top_k: int = 10, 
              filter_dict: dict = None) -> list[dict]:
        """
        Query the vector database with semantic search.
        
        Query Process:
        1. Convert query text to embedding using RETRIEVAL_QUERY
        2. Calculate cosine similarity against all stored vectors
        3. Return top K most similar results with metadata
        
        Args:
            query_text: Natural language search query
            top_k: Number of results to return (default: 10)
            filter_dict: Optional metadata filter (e.g., {"sheet_name": "Overrides"})
            
        Returns:
            list[dict]: Matches sorted by similarity score (descending)
                - id: Vector ID (e.g., "overrides_row_15")
                - score: Cosine similarity (0.0 to 1.0, higher = more similar)
                - metadata: Original row data and sheet information
                
        Example:
            >>> results = service.query("how to configure settings", top_k=5)
            >>> for r in results:
            ...     print(f"{r['score']:.3f} - {r['metadata']['sheet_name']}")
        """
        query_embedding = self.embed_query(query_text)
        
        query_params = {
            "vector": query_embedding,
            "top_k": top_k,
            "include_metadata": True
        }
        
        if filter_dict:
            query_params["filter"] = filter_dict
        
        results = self.index.query(**query_params)
        
        return [
            {
                "id": match.id,
                "score": match.score,
                "metadata": match.metadata
            }
            for match in results.matches
        ]

    def query_by_sheet(self, query_text: str, sheet_name: str, 
                       top_k: int = 10) -> list[dict]:
        """
        Query vectors filtered by a specific Excel sheet.
        
        Convenience method for searching within a single sheet.
        
        Args:
            query_text: Search query
            sheet_name: Exact sheet name (e.g., "Master Question List")
            top_k: Number of results
            
        Returns:
            list[dict]: Matching vectors from the specified sheet only
        """
        return self.query(
            query_text=query_text,
            top_k=top_k,
            filter_dict={"sheet_name": sheet_name}
        )

